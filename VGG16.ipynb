{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import os\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets, models\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from PIL import Image\n",
    "from torch.utils.data.sampler import SubsetRandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiabeticRetinopathyDataset(Dataset):\n",
    "    def __init__(self, base_dir, transform=None):\n",
    "        self.base_dir = base_dir\n",
    "        self.transform = transform\n",
    "        self.data = []\n",
    "        categories = ['No_DR', 'Mild', 'Moderate', 'Severe', 'Proliferate_DR']\n",
    "\n",
    "        for idx, category in enumerate(categories):\n",
    "            path = os.path.join(base_dir, category)\n",
    "            images = os.listdir(path)[:1000]  # Take only 1000 images per category\n",
    "            for image in images:\n",
    "                self.data.append((os.path.join(path, image), idx))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.data[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Assuming 'base_dir' is your dataset directory path\n",
    "base_dir = 'data/gaussian_filtered_images/gaussian_filtered_images'  # Update this path\n",
    "dataset = DiabeticRetinopathyDataset(base_dir=base_dir, transform=transform)\n",
    "\n",
    "# Splitting the dataset into train and validation\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pavankumardharmoju/anaconda3/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/pavankumardharmoju/anaconda3/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model = models.vgg16(pretrained=True)\n",
    "\n",
    "# Freeze model parameters\n",
    "for param in model.features.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Modify the classifier\n",
    "num_features = model.classifier[6].in_features\n",
    "features = list(model.classifier.children())[:-1] # Remove last layer\n",
    "features.extend([nn.Linear(num_features, 5)]) # Add our layer with 5 outputs\n",
    "model.classifier = nn.Sequential(*features) # Replace the model classifier\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.classifier.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 1.3180769317679935, Validation Loss: 0.8306207160154978, Validation Accuracy: 68.53146853146853%\n",
      "Epoch 2, Train Loss: 1.0767698751555548, Validation Loss: 0.8598845733536614, Validation Accuracy: 71.15384615384616%\n",
      "Epoch 3, Train Loss: 1.1011504009366035, Validation Loss: 0.969506926006741, Validation Accuracy: 67.13286713286713%\n",
      "Epoch 4, Train Loss: 1.049792666402128, Validation Loss: 0.8800554407967461, Validation Accuracy: 71.5034965034965%\n",
      "Epoch 5, Train Loss: 1.0093780640098784, Validation Loss: 0.8800578051143222, Validation Accuracy: 70.1048951048951%\n",
      "Epoch 6, Train Loss: 0.9827726926240656, Validation Loss: 0.780158422059483, Validation Accuracy: 70.27972027972028%\n",
      "Epoch 7, Train Loss: 0.8756736864646276, Validation Loss: 0.8600136107868619, Validation Accuracy: 72.9020979020979%\n",
      "Epoch 8, Train Loss: 0.8790322182079157, Validation Loss: 0.8996715380085839, Validation Accuracy: 71.32867132867133%\n",
      "Epoch 9, Train Loss: 0.8878432847559452, Validation Loss: 0.9092681507269541, Validation Accuracy: 70.8041958041958%\n",
      "Epoch 10, Train Loss: 0.8769352498153845, Validation Loss: 0.9507540596856011, Validation Accuracy: 69.05594405594405%\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    print(f'Epoch {epoch+1}, Train Loss: {running_loss / len(train_loader)}, Validation Loss: {val_loss / len(val_loader)}, Validation Accuracy: {100 * correct / total}%')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
